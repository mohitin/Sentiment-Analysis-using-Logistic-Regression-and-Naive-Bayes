{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI4nu1nA_Phf"
      },
      "source": [
        "# Sentiment Analysis using Logistic Regression and Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "TBmBNUQfRZ5C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "v3HoRSTG_Phl"
      },
      "outputs": [],
      "source": [
        "## importing libraries\n",
        "import nltk                                \n",
        "from nltk.corpus import twitter_samples   \n",
        "import matplotlib.pyplot as plt            \n",
        "import random    \n",
        "import numpy as np\n",
        "import re                                  \n",
        "import string                              \n",
        "# nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords          \n",
        "from nltk.stem import PorterStemmer        \n",
        "from nltk.tokenize import TweetTokenizer  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENn3r7uh_Phn"
      },
      "source": [
        "### About the dataset\n",
        "\n",
        "The sample dataset from NLTK is separated into positive and negative tweets. It contains 5000 positive tweets and 5000 negative tweets exactly.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB61356z_Pho",
        "outputId": "853bc05c-e827-4f77-933d-95d36984af35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# downloads sample twitter dataset.\n",
        "nltk.download('twitter_samples')\n",
        "## selecting positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "VPfnf3w5_Php",
        "outputId": "dd11e0d8-be39-4570-8ed6-462691ed63f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of positive tweets:  5000\n",
            "Number of negative tweets:  5000\n",
            "\n",
            "The type of all_positive_tweets is:  <class 'list'>\n",
            "The type of a tweet entry is:  <class 'str'>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# select the set of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "## reporting the number\n",
        "print('Number of positive tweets: ', len(all_positive_tweets))\n",
        "print('Number of negative tweets: ', len(all_negative_tweets))\n",
        "print('\\nThe type of all_positive_tweets is: ', type(all_positive_tweets))\n",
        "print('The type of a tweet entry is: ', type(all_negative_tweets[0]))\n",
        "\n",
        "## displaying the piechart of the numbers\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "labels = 'Positives', 'Negative'\n",
        "sizes = [len(all_positive_tweets), len(all_negative_tweets)] \n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
        "        shadow=True, startangle=90)\n",
        "plt.axis('equal')  \n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into two pieces, one for training and one for testing (validation set) \n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg \n",
        "test_x = test_pos + test_neg\n",
        "## creating labels\n",
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)\n",
        "\n",
        "print(\"train_y.shape = \" + str(train_y.shape))\n",
        "print(\"test_y.shape = \" + str(test_y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AU6oIcIc-PEG",
        "outputId": "9ee40f08-d4c4-4cc4-a80d-d6da67b8924b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_y.shape = (8000, 1)\n",
            "test_y.shape = (2000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImATxST1_Phu"
      },
      "source": [
        "### Preprocessing raw text for Sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azjelL6u_Phv"
      },
      "source": [
        "Task:\n",
        "* Tokenizing the string\n",
        "* Lowercasing\n",
        "* Removing stop words and punctuation\n",
        "* Stemming\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGjA0-tR_Phv",
        "outputId": "57373273-a1fe-4520-8466-c424b9cd1c39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive tweet (w/o preprocessing) : @jayyycgee Well you're about to get recruited to Team Doom. So The Nasty Crew is going to be affiliated. &gt;:)\n",
            "Negative tweet (w/o preprocessing) : @stormieraae what the heck :( you don't follow her?\n",
            "\n",
            "My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n",
            "\n",
            "My beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… \n"
          ]
        }
      ],
      "source": [
        "## Printing example tweets\n",
        "tweet=all_positive_tweets[2277]\n",
        "print(f'Positive tweet (w/o preprocessing) : {all_positive_tweets[2000]}')\n",
        "print(f'Negative tweet (w/o preprocessing) : {all_negative_tweets[2000]}')\n",
        "print()\n",
        "## Removing hyperlinks, twitter marks and styles\n",
        "print(tweet)\n",
        "print()\n",
        "# remove old style retweet text \"RT\"\n",
        "tweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "# remove hyperlinks\n",
        "tweet2 = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet2)\n",
        "# remove hashtags\n",
        "tweet2 = re.sub(r'#', '', tweet2)\n",
        "print(tweet2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85bgS3U6_Phx"
      },
      "source": [
        "Tokenize the string\n",
        "\n",
        "To tokenize means to split the strings into individual words without blanks or tabs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Sjev--s_Phx",
        "outputId": "d7882ca8-7a0f-433e-9895-ae86822b8926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… \n",
            "\n",
            "Tokenized string:\n",
            "['my', 'beautiful', 'sunflowers', 'on', 'a', 'sunny', 'friday', 'morning', 'off', ':)', 'sunflowers', 'favourites', 'happy', 'friday', 'off', '…']\n"
          ]
        }
      ],
      "source": [
        "print(tweet2)\n",
        "# instantiate tokenizer class\n",
        "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "# tokenize tweets\n",
        "tweet_tokens = tokenizer.tokenize(tweet2)\n",
        "print()\n",
        "print('Tokenized string:')\n",
        "print(tweet_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO1gMc-O_Phx"
      },
      "source": [
        "Remove stop words and punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbmKpcW3_Phy",
        "outputId": "850ec707-30ea-4003-a913-c3d9a5d680fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop words: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "Punctuation: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "\n",
            "\n",
            "removed stop words and punctuation:\n",
            "['beautiful', 'sunflowers', 'sunny', 'friday', 'morning', ':)', 'sunflowers', 'favourites', 'happy', 'friday', '…']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#Import the english stop words list from NLTK\n",
        "nltk.download('stopwords')\n",
        "stopwords_english = stopwords.words('english') \n",
        "\n",
        "print(f'Stop words: {stopwords_english}')\n",
        "print(f'Punctuation: {string.punctuation}')\n",
        "print()\n",
        "# print(tweet_tokens)\n",
        "\n",
        "tweets_clean = []\n",
        "for word in tweet_tokens: \n",
        "    if (word not in stopwords_english and word not in string.punctuation):  \n",
        "        tweets_clean.append(word)\n",
        "print()\n",
        "print('removed stop words and punctuation:')\n",
        "print(tweets_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPhL29t0_Phy"
      },
      "source": [
        "Please note that the words **happy** and **sunny** in this list are correctly spelled. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYkvNIfj_Phz"
      },
      "source": [
        "Stemming: \\\n",
        "Stemming is the process of converting a word to its most general form, or stem. This helps in reducing the size of our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mk1BD2cs_Phz",
        "outputId": "d7cacd26-a266-4d57-c2e1-0de5fd122660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['beautiful', 'sunflowers', 'sunny', 'friday', 'morning', ':)', 'sunflowers', 'favourites', 'happy', 'friday', '…']\n",
            "stemmed words:\n",
            "['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '…']\n"
          ]
        }
      ],
      "source": [
        "print(tweets_clean)\n",
        "# Instantiate stemming class\n",
        "stemmer = PorterStemmer() \n",
        "tweets_stem = [] \n",
        "for word in tweets_clean:\n",
        "    stem_word = stemmer.stem(word) \n",
        "    tweets_stem.append(stem_word)\n",
        "print('stemmed words:')\n",
        "print(tweets_stem)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create frequency dictionary\n",
        "freqs = build_freqs(train_x, train_y)\n",
        "# check the output\n",
        "print(\"type(freqs) = \" + str(type(freqs)))\n",
        "print(\"len(freqs) = \" + str(len(freqs.keys())))\n",
        "\n",
        "#test the function below\n",
        "print('This is an example of a positive tweet: \\n', train_x[0])\n",
        "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEMB5D_J_XNL",
        "outputId": "5a562824-3c1d-4f6c-f523-aeb5189fc41f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type(freqs) = <class 'dict'>\n",
            "len(freqs) = 11428\n",
            "This is an example of a positive tweet: \n",
            " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "\n",
            "This is an example of the processed version of the tweet: \n",
            " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression Implementation"
      ],
      "metadata": {
        "id": "x4jl1zoxChZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Functions"
      ],
      "metadata": {
        "id": "cURXq55ZKpnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_freqs(tweets, ys):\n",
        "  \n",
        "    yslist = np.squeeze(ys).tolist()\n",
        "    freqs = {}\n",
        "    for y, tweet in zip(yslist, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            pair = (word, y)\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1    \n",
        "    return freqs\n",
        "def process_tweet(tweet):\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks    \n",
        "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "                word not in string.punctuation):  # remove punctuation\n",
        "            # tweets_clean.append(word)\n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean\n",
        "\n",
        "def extract_features(tweet, freqs):\n",
        "    # process_tweet tokenizes, stems, and removes stopwords\n",
        "    word_l = process_tweet(tweet)\n",
        "    \n",
        "    # 3 elements in the form of a 1 x 3 vector\n",
        "    x = np.zeros((1, 3)) \n",
        "    \n",
        "    #bias term is set to 1\n",
        "    x[0,0] = 1 \n",
        "    \n",
        "    # loop through each word in the list of words\n",
        "    for word in word_l:     \n",
        "        # increment the word count for the positive label 1\n",
        "        x[0,1] += freqs.get((word, 1.0),0)    \n",
        "        # increment the word count for the negative label 0\n",
        "        x[0,2] += freqs.get((word, 0.0),0)\n",
        "    assert(x.shape == (1, 3))\n",
        "    return x"
      ],
      "metadata": {
        "id": "Yg0vUI6GfoVt"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z): \n",
        "    # calculate the sigmoid of z\n",
        "    h = 1 / (1 + np.exp(-z))\n",
        "    return h\n",
        "  \n",
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "    m = x.shape[0]\n",
        "    for i in range(0, num_iters): \n",
        "        z = np.dot(x,theta)      \n",
        "        # get the sigmoid of z\n",
        "        h = sigmoid(z)\n",
        "        # calculate the cost function\n",
        "        J = -1./m * (np.dot(y.transpose(), np.log(h)) + np.dot((1-y).transpose(),np.log(1-h)))    \n",
        "        # update the weights theta\n",
        "        theta = theta = theta - (alpha/m) * np.dot(x.transpose(),(h-y))\n",
        "    J = float(J)\n",
        "    return J, theta\n",
        "\n",
        "# collect the features 'x' and stack them into a matrix 'X'\n",
        "X = np.zeros((len(train_x), 3))\n",
        "for i in range(len(train_x)):\n",
        "    X[i, :]= extract_features(train_x[i], freqs)\n",
        "\n",
        "# training labels corresponding to X\n",
        "Y = train_y\n",
        "\n",
        "# Apply gradient descent\n",
        "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
        "print(f\"The cost after training is {J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-PBkrYGC8nj",
        "outputId": "d3911c03-525e-468a-cc3b-0f297f5694e2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cost after training is 0.22521260.\n",
            "The resulting vector of weights is [6e-08, 0.0005382, -0.0005583]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the predictions"
      ],
      "metadata": {
        "id": "jbH1Ez_LGNrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_tweet(tweet, freqs, theta):\n",
        "    x = extract_features(tweet,freqs)\n",
        "    y_pred = sigmoid(np.dot(x,theta))\n",
        "    \n",
        "    return y_pred\n",
        "\n",
        "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
        "    # the list for storing predictions\n",
        "    y_hat = []\n",
        "    \n",
        "    for tweet in test_x:\n",
        "        # get the label prediction for the tweet\n",
        "        y_pred = predict_tweet(tweet, freqs, theta)\n",
        "        \n",
        "        if y_pred > 0.5:\n",
        "            # append 1.0 to the list\n",
        "            y_hat.append(1)\n",
        "        else:\n",
        "            # append 0 to the list\n",
        "            y_hat.append(0)\n",
        "    accuracy = (y_hat==np.squeeze(test_y)).sum()/len(test_x)\n",
        "    return accuracy\n",
        "\n",
        "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
        "print(f\"Logistic regression model's accuracy = {tmp_accuracy*100:.2f}\")\n",
        "# Feel free to change the tweet below\n",
        "my_tweet = 'Sad to hear that'\n",
        "print(process_tweet(my_tweet))\n",
        "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
        "print(y_hat)\n",
        "if y_hat > 0.5:\n",
        "    print('Positive sentiment')\n",
        "else: \n",
        "    print('Negative sentiment')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VjD0GAjDfg2",
        "outputId": "898176e6-e9b2-4412-9517-de47d9cdc6a3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression model's accuracy = 99.50\n",
            "['sad', 'hear']\n",
            "[[0.4863188]]\n",
            "Negative sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes Implementation"
      ],
      "metadata": {
        "id": "7RxpwOmZLMgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## importing the required librairies\n",
        "import pdb\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from os import getcwd\n",
        "nltk.download('stopwords')\n",
        "nltk.download('twitter_samples')\n",
        "\n",
        "# get the sets of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "# split the data into two pieces, one for training and one for testing (validation set)\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000] \n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg\n",
        "\n",
        "# avoid assumptions about the length of all_positive_tweets\n",
        "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
        "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf4CnRJENlvl",
        "outputId": "c1bb649c-7720-43ab-ba42-f087789f66b3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions"
      ],
      "metadata": {
        "id": "xVp5X0o3LM5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_naive_bayes(freqs, train_x, train_y):\n",
        "    loglikelihood = {}\n",
        "    vocab = set([pair[0] for pair in freqs.keys()])\n",
        "    V = len(vocab)\n",
        "    # calculate N_pos and N_neg\n",
        "    N_pos = N_neg = 0\n",
        "    for pair in freqs.keys():\n",
        "        # if the label is positive (greater than zero)\n",
        "        if pair[1] > 0:\n",
        "            N_pos += freqs.get(pair, 1)\n",
        "        else:\n",
        "            N_neg += freqs.get(pair, 1)\n",
        "    D = len(train_y)\n",
        "    D_pos = sum(train_y)\n",
        "    D_neg = D-D_pos\n",
        "    # Calculate logprior\n",
        "    logprior = np.log(D_pos) - np.log(D_neg)\n",
        "    for word in vocab:\n",
        "        freq_pos = freqs.get((word, 1),0)\n",
        "        freq_neg = freqs.get((word, 0),0)\n",
        "        p_w_pos = (freq_pos + 1)/(N_pos + V)\n",
        "        p_w_neg = (freq_neg + 1)/(N_neg + V)\n",
        "        loglikelihood[word] = np.log(p_w_pos/p_w_neg)\n",
        "    return logprior, loglikelihood\n",
        "\n",
        "def count_tweets(result, tweets, ys):\n",
        "    for y, tweet in zip(ys, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            pair = (word,y)\n",
        "            if pair in result:\n",
        "                result[pair] += 1\n",
        "            else:\n",
        "                result[pair] = 1\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "4GNRBomXLL5z"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = {}\n",
        "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
        "ys = [1, 0, 0, 0, 0]\n",
        "count_tweets(result, tweets, ys)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcTe9BfWPhpD",
        "outputId": "35a6fcbf-4bb3-446f-acc4-44d24b3ce3b3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('happi', 1): 1, ('sad', 0): 1, ('tire', 0): 2, ('trick', 0): 1}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freqs = count_tweets({}, train_x, train_y)\n",
        "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
        "print(logprior)\n",
        "print(len(loglikelihood))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zS1TbBqMl9B",
        "outputId": "c52e8780-8239-44e3-f2a5-8faf3008b913"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "7570\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the predictions"
      ],
      "metadata": {
        "id": "yuq_EB1EQ376"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
        "   \n",
        "    # process the tweet to get a list of words\n",
        "    word_l = process_tweet(tweet)\n",
        "# initialize probability to zero\n",
        "    p = 0\n",
        "# add the logprior\n",
        "    p += logprior\n",
        "    for word in word_l:\n",
        "    # check if the word exists in the loglikelihood dictionary\n",
        "            if word in loglikelihood:\n",
        "                # add the log likelihood of that word to the probability\n",
        "                p += loglikelihood[word]\n",
        "    return p\n",
        "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
        "    accuracy = 0\n",
        "    y_hats = []\n",
        "    for tweet in test_x:\n",
        "        # if the prediction is > 0\n",
        "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
        "            # the predicted class is 1\n",
        "            y_hat_i = 1\n",
        "        else:\n",
        "            y_hat_i = 0\n",
        "        y_hats.append(y_hat_i)\n",
        "    \n",
        "    error = np.mean(np.absolute(y_hats - test_y))\n",
        "    accuracy = 1 - error\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "bw7LwKEOLoT5"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_tweet = 'This is awful.'\n",
        "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
        "print('The expected output is', p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp4_eeLuOjdZ",
        "outputId": "6f24893c-4d52-4aac-cc31-277147705a05"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The expected output is -2.440207463180464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pred=naive_bayes_predict(logprior,loglikelihood)\n",
        "accuracy_naive = test_naive_bayes(test_x, test_y, logprior, loglikelihood)\n",
        "print(f\"Naive bayes model's accuracy = {accuracy_naive*100:.2f}%\")\n",
        "# Feel free to change the tweet below\n",
        "my_tweet = 'I am not happy but sad'\n",
        "print(process_tweet(my_tweet))\n",
        "yHat = naive_bayes_predict(my_tweet,logprior,loglikelihood)\n",
        "print(yHat)\n",
        "if yHat > 0.5:\n",
        "    print('Positive sentiment')\n",
        "else: \n",
        "    print('Negative sentiment')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDCNjgYsL7YE",
        "outputId": "c8a21983-ccf4-49fa-a86e-fd2792471f52"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive bayes model's accuracy = 99.45%\n",
            "['happi', 'sad']\n",
            "-0.7813332638545585\n",
            "Negative sentiment\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Sentiment_analysis_LR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}